{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0V-FhInzOuO"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0qcE9Fk3yVN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "from glob import glob\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.regularizers import l2\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nQ-jba5346"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jWSVA215zh6",
        "outputId": "ab1ced1f-24fe-4a08-89a3-ac30f13f4db8"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "base_dir = '/content/drive/MyDrive/TB-Classification/tbx11k_dataset'\n",
        "print(os.path.exists(base_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvWpHFSEzMDa"
      },
      "source": [
        "## Paths and Directories\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTzIIBD9zFwr",
        "outputId": "8901e920-13ea-4f85-c365-109c0ab0e131"
      },
      "outputs": [],
      "source": [
        "source_dir = '/content/drive/MyDrive/TB-Classification/tbx11k_dataset'\n",
        "local_dir = '/content/tbx11k_dataset'\n",
        "\n",
        "def count_files(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        return 0\n",
        "    return sum([len(files) for r, d, files in os.walk(directory)])\n",
        "\n",
        "print(f\"Checking dataset integrity...\")\n",
        "\n",
        "num_source_files = count_files(os.path.join(source_dir, 'train'))\n",
        "print(f\"Source (Drive) has {num_source_files} training files.\")\n",
        "\n",
        "num_local_files = count_files(os.path.join(local_dir, 'train'))\n",
        "print(f\"Local (Colab) has {num_local_files} training files.\")\n",
        "\n",
        "if num_local_files != num_source_files or num_source_files == 0:\n",
        "    print(\"Mismatch or empty folder detected. Deleting and re-copying...\")\n",
        "\n",
        "    if os.path.exists(local_dir):\n",
        "        shutil.rmtree(local_dir)\n",
        "\n",
        "    print(\"Starting fresh copy...\")\n",
        "    shutil.copytree(source_dir, local_dir)\n",
        "    print(\"Copy complete!\")\n",
        "else:\n",
        "    print(\"Dataset is complete and matches source. Skipping copy.\")\n",
        "\n",
        "BASE_DIR = local_dir\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
        "TRAIN_ANN_DIR = os.path.join(BASE_DIR, 'train', 'ann')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val')\n",
        "MODELS_DIR = '/content/drive/MyDrive/TB-Classification/tbx11k_dataset/models'\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "checkpoint_path = os.path.join(MODELS_DIR, 'best_tb_model.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4maBiSTr3vi"
      },
      "source": [
        "## Dataset Mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfQCEPf4r5OG",
        "outputId": "0d0c6bb0-2611-4072-9a42-67008ac6d0e2"
      },
      "outputs": [],
      "source": [
        "LABEL_MAP = {\n",
        "    'healthy': 'Healthy',\n",
        "    'sick_but_non-tb': 'Sick_but_Non_TB',\n",
        "    'active_tb': 'Active_TB',\n",
        "    'latent_tb': 'Latent_TB',\n",
        "    'active&latent_tb': 'Latent_TB',\n",
        "    'UNKNOWN': None\n",
        "}\n",
        "\n",
        "TARGET_CLASSES = ['Healthy', 'Sick_but_Non_TB', 'Active_TB', 'Latent_TB']\n",
        "\n",
        "def create_dataframe(directory):\n",
        "    img_dir = os.path.join(directory, 'img')\n",
        "    ann_dir = os.path.join(directory, 'ann')\n",
        "    data = []\n",
        "\n",
        "    if not os.path.exists(ann_dir):\n",
        "        print(f\"Warning: Annotation directory not found: {ann_dir}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    json_files = glob(os.path.join(ann_dir, '*.json'))\n",
        "    print(f\"Scanning {len(json_files)} annotations in {directory}...\")\n",
        "\n",
        "    for j_file in json_files:\n",
        "        try:\n",
        "            with open(j_file, 'r') as f:\n",
        "                content = json.load(f)\n",
        "\n",
        "            raw_label = None\n",
        "            if 'tags' in content and len(content['tags']) > 0:\n",
        "                raw_label = content['tags'][0].get('name')\n",
        "            elif 'objects' in content and len(content['objects']) > 0:\n",
        "                raw_label = content['objects'][0].get('classTitle')\n",
        "\n",
        "            if not raw_label: continue\n",
        "\n",
        "            mapped_label = LABEL_MAP.get(raw_label)\n",
        "            if mapped_label is None or mapped_label not in TARGET_CLASSES:\n",
        "                continue\n",
        "\n",
        "            base_name = os.path.basename(j_file)\n",
        "            for ext in ['.json', '.png', '.jpg', '.jpeg']:\n",
        "                base_name = base_name.replace(ext, '')\n",
        "\n",
        "            img_path = None\n",
        "            for ext in ['.png', '.jpg', '.jpeg']:\n",
        "                temp_path = os.path.join(img_dir, base_name + ext)\n",
        "                if os.path.exists(temp_path):\n",
        "                    img_path = temp_path\n",
        "                    break\n",
        "\n",
        "            if img_path:\n",
        "                data.append({'filename': img_path, 'class': mapped_label})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {j_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"Found {len(df)} images in {directory}\")\n",
        "    return df\n",
        "\n",
        "LABEL_MAP = {\n",
        "    'healthy': 'Healthy',\n",
        "    'sick_but_non-tb': 'Sick_but_Non_TB',\n",
        "    'active_tb': 'Active_TB',\n",
        "    'latent_tb': 'Latent_TB',\n",
        "    'active&latent_tb': 'Latent_TB',\n",
        "    'UNKNOWN': None\n",
        "}\n",
        "TARGET_CLASSES = ['Healthy', 'Sick_but_Non_TB', 'Active_TB', 'Latent_TB']\n",
        "\n",
        "print(\"--- Loading Data ---\")\n",
        "train_df = create_dataframe(TRAIN_DIR)\n",
        "val_df = create_dataframe(VAL_DIR)\n",
        "\n",
        "if train_df.empty:\n",
        "    raise ValueError(\"CRITICAL ERROR: No training data found. Check your TRAIN_DIR path.\")\n",
        "\n",
        "print(f\"\\n--- Class Distribution BEFORE Balancing ---\\n{train_df['class'].value_counts()}\")\n",
        "\n",
        "target_n = 2000\n",
        "\n",
        "def balanced_sample(df_subset, target_n):\n",
        "    if len(df_subset) == 0:\n",
        "        return df_subset\n",
        "    if len(df_subset) < target_n:\n",
        "        print(f\"Upsampling {df_subset['class'].iloc[0]} from {len(df_subset)} to {target_n}\")\n",
        "        return resample(df_subset, replace=True, n_samples=target_n, random_state=42)\n",
        "\n",
        "    else:\n",
        "        print(f\"Downsampling {df_subset['class'].iloc[0]} from {len(df_subset)} to {target_n}\")\n",
        "        return resample(df_subset, replace=False, n_samples=target_n, random_state=42)\n",
        "\n",
        "df_healthy = train_df[train_df['class'] == 'Healthy']\n",
        "df_sick = train_df[train_df['class'] == 'Sick_but_Non_TB']\n",
        "df_active = train_df[train_df['class'] == 'Active_TB']\n",
        "df_latent = train_df[train_df['class'] == 'Latent_TB']\n",
        "\n",
        "train_df_balanced = pd.concat([\n",
        "    balanced_sample(df_healthy, 3000),\n",
        "    balanced_sample(df_sick, 3000),\n",
        "    balanced_sample(df_active, target_n),\n",
        "    balanced_sample(df_latent, target_n)\n",
        "])\n",
        "\n",
        "train_df = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n--- Class Distribution AFTER Balancing ---\\n{train_df['class'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqoE1HPHyNpJ"
      },
      "source": [
        "## Configuration and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trWdP5OwyE1H"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"IMG_HEIGHT\": 224,\n",
        "    \"IMG_WIDTH\": 224,\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"EPOCHS\": 20,\n",
        "    \"LEARNING_RATE\": 1e-4,\n",
        "    \"DROPOUT_RATE\": 0.5,\n",
        "    \"L2_REG\": 0.001\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUhTsrINzJTg"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN98L40HzGdS",
        "outputId": "f5a3b12c-4cf2-485a-915d-02d0f7928523"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(\n",
        "    rescale=1./255\n",
        ")\n",
        "\n",
        "print(\"Data Generators created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_qGQI0eBHCm"
      },
      "source": [
        "## Generators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTRvpoLBIhx",
        "outputId": "7f5631f0-0f85-4a11-a3e0-04ba3fb12d14"
      },
      "outputs": [],
      "source": [
        "print(\"Initializing Data Generators from DataFrames...\")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(CONFIG[\"IMG_HEIGHT\"], CONFIG[\"IMG_WIDTH\"]),\n",
        "    batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "    class_mode='categorical',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='filename',\n",
        "    y_col='class',\n",
        "    target_size=(CONFIG[\"IMG_HEIGHT\"], CONFIG[\"IMG_WIDTH\"]),\n",
        "    batch_size=CONFIG[\"BATCH_SIZE\"],\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM1UGykM7WSE"
      },
      "source": [
        "## Imbalance Handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQHl5RXD7Y1d",
        "outputId": "bb05d21c-6d10-44ea-d5b2-7f29948ac92b"
      },
      "outputs": [],
      "source": [
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_generator.classes),\n",
        "    y=train_generator.classes\n",
        ")\n",
        "train_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "print(f\"Class Weights: {train_class_weights}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PREM5tCgbM8s"
      },
      "source": [
        "## Focal Loss Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BedA9xnbKGZ"
      },
      "outputs": [],
      "source": [
        "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
        "    \"\"\"\n",
        "    Implementation of Focal Loss from Lin et al., 2017.\n",
        "    Arguments:\n",
        "        gamma (float): Focusing parameter. Higher values focus more on hard examples.\n",
        "        alpha (float): Balancing parameter.\n",
        "    \"\"\"\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n",
        "\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "\n",
        "        weight = alpha * y_true * K.pow((1 - y_pred), gamma)\n",
        "\n",
        "        loss = weight * cross_entropy\n",
        "        return K.sum(loss, axis=1)\n",
        "\n",
        "    return focal_loss_fixed\n",
        "\n",
        "tf.keras.utils.get_custom_objects().update({'focal_loss_fixed': categorical_focal_loss})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFIeNWE3-Cgm"
      },
      "source": [
        "## VGG-Style CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "ZMHEd2TL-N6a",
        "outputId": "306123bf-04ec-4bf9-ba9a-7e05bdfaa005"
      },
      "outputs": [],
      "source": [
        "def build_tuned_cnn():\n",
        "    regularizer = l2(CONFIG[\"L2_REG\"])\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(CONFIG[\"IMG_HEIGHT\"], CONFIG[\"IMG_WIDTH\"], 3)),\n",
        "\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        Conv2D(512, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizer),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        GlobalAveragePooling2D(),\n",
        "\n",
        "        Dense(256, activation='relu', kernel_regularizer=regularizer),\n",
        "        Dropout(CONFIG[\"DROPOUT_RATE\"]),\n",
        "\n",
        "        Dense(128, activation='relu', kernel_regularizer=regularizer),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(4, activation='softmax', dtype='float32')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "resume_checkpoint_path = os.path.join(MODELS_DIR, 'best_tb_model.keras')\n",
        "\n",
        "if os.path.exists(resume_checkpoint_path):\n",
        "    print(f\"Found checkpoint at: {resume_checkpoint_path}\")\n",
        "    print(\"Resuming training...\")\n",
        "    model = load_model(resume_checkpoint_path, custom_objects={'focal_loss_fixed': categorical_focal_loss()})\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    print(f\"âš ï¸ No checkpoint found. Building fresh model...\")\n",
        "    model = build_tuned_cnn()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG[\"LEARNING_RATE\"])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=categorical_focal_loss(gamma=2.0, alpha=0.25),\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.Precision(name='precision')\n",
        "        ]\n",
        "    )\n",
        "    print(\"âœ… New model compiled with Focal Loss and Recall metrics.\")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1euc-Tj3-Q9L"
      },
      "source": [
        "## Training and Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d81qWJfm-dak",
        "outputId": "778f472d-d24c-40e8-c4d1-68d2db6a2cb5"
      },
      "outputs": [],
      "source": [
        "LOGS_DIR = os.path.join(BASE_DIR, 'logs')\n",
        "os.makedirs(LOGS_DIR, exist_ok=True)\n",
        "\n",
        "log_path = os.path.join(LOGS_DIR, 'training_log.csv')\n",
        "best_model_path = os.path.join(MODELS_DIR, 'best_tb_model.keras')\n",
        "last_model_path = os.path.join(MODELS_DIR, 'last_tb_model.keras')\n",
        "\n",
        "print(f\"ðŸ“‚ Logs directory: {LOGS_DIR}\")\n",
        "print(f\"ðŸ’¾ Best Model path: {best_model_path}\")\n",
        "print(f\"ðŸ’¾ Last Model path: {last_model_path}\")\n",
        "\n",
        "\n",
        "# --- STEP 1: Determine Start Epoch ---\n",
        "\n",
        "initial_epoch = 0\n",
        "\n",
        "if os.path.exists(log_path):\n",
        "    try:\n",
        "        log_df = pd.read_csv(log_path)\n",
        "        if not log_df.empty:\n",
        "            # The CSV logs epochs starting from 0.\n",
        "            # If the max epoch in CSV is 5, we have finished epoch 5.\n",
        "            # So we should start at 6.\n",
        "            last_recorded_epoch = log_df['epoch'].max()\n",
        "            initial_epoch = last_recorded_epoch + 1\n",
        "            print(f\"ðŸ“„ Log file found. Resuming from Epoch {initial_epoch} (0-indexed).\")\n",
        "        else:\n",
        "            print(\"ðŸ“„ Log file is empty. Starting from Epoch 0.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error reading log file: {e}. Starting from Epoch 0.\")\n",
        "else:\n",
        "    print(\"ðŸ“„ No log file found. Starting from Epoch 0.\")\n",
        "\n",
        "\n",
        "# --- STEP 2: Define Callbacks ---\n",
        "\n",
        "# 1. Logging Callback (Append mode is crucial for resuming)\n",
        "csv_logger = CSVLogger(log_path, append=True)\n",
        "\n",
        "# 2. Checkpointing - BEST Model\n",
        "# This saves the model only when validation accuracy improves.\n",
        "checkpoint_best = ModelCheckpoint(\n",
        "    best_model_path,\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 3. Checkpointing - LAST Model (Crucial for Resuming)\n",
        "# This saves the model at the end of *every* epoch.\n",
        "# If training crashes, this file represents the most recent state.\n",
        "checkpoint_last = ModelCheckpoint(\n",
        "    last_model_path,\n",
        "    save_best_only=False, # Save regardless of performance\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# 4. Dynamic Learning Rate\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,       # Changed from 0.5 to 0.2 (more aggressive drop)\n",
        "    patience=3,       # Increased patience slightly\n",
        "    min_lr=1e-7,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 5. Early Stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "\n",
        "# --- STEP 3: Start Training ---\n",
        "# Only run fit if we haven't reached the max epochs yet\n",
        "if initial_epoch < CONFIG[\"EPOCHS\"]:\n",
        "    print(f\"ðŸš€ Starting Training...\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // CONFIG[\"BATCH_SIZE\"],\n",
        "        epochs=CONFIG[\"EPOCHS\"],\n",
        "        initial_epoch=initial_epoch,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.samples // CONFIG[\"BATCH_SIZE\"],\n",
        "        # class_weight=train_class_weights,  <-- REMOVE THIS LINE. We used oversampling instead.\n",
        "        callbacks=[early_stop, reduce_lr, checkpoint_best, checkpoint_last, csv_logger]\n",
        "    )\n",
        "    print(\"Training Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kjrri6Ym-kAx"
      },
      "source": [
        "## Report Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dUPnsNSP-mLA",
        "outputId": "8e92e366-ce95-40a1-9f50-407f05c326de"
      },
      "outputs": [],
      "source": [
        "def generate_report(model, val_gen, output_path):\n",
        "    print(\"Generating predictions (this may take a moment)...\")\n",
        "\n",
        "    # 1. Predictions\n",
        "    # Reset generator to start so predictions match labels exactly\n",
        "    val_gen.reset()\n",
        "\n",
        "    # Calculate exact steps to ensure we cover all images (using math.ceil)\n",
        "    steps = math.ceil(val_gen.samples / val_gen.batch_size)\n",
        "\n",
        "    # Predict\n",
        "    Y_pred = model.predict(val_gen, steps=steps, verbose=1)\n",
        "\n",
        "    # Convert probabilities (e.g., [0.1, 0.8, 0.1]) to class ID (e.g., 1)\n",
        "    y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "    # Get True labels (val_generator must have shuffle=False)\n",
        "    y_true = val_gen.classes\n",
        "\n",
        "    # Get Class Names\n",
        "    labels = list(val_gen.class_indices.keys())\n",
        "\n",
        "    # 2. Confusion Matrix Plot\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Class')\n",
        "    plt.xlabel('Predicted Class')\n",
        "\n",
        "    cm_path = os.path.join(output_path, 'confusion_matrix.png')\n",
        "    plt.savefig(cm_path)\n",
        "    print(f\"Confusion Matrix saved to {cm_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Text Report (Precision, Recall, F1)\n",
        "    report = classification_report(y_true, y_pred, target_names=labels)\n",
        "    report_path = os.path.join(output_path, 'classification_report.txt')\n",
        "\n",
        "    with open(report_path, \"w\") as f:\n",
        "        f.write(\"FINAL PROJECT EVALUATION REPORT\\n\")\n",
        "        f.write(\"===============================\\n\")\n",
        "        f.write(f\"Model: Custom CNN (From Scratch)\\n\")\n",
        "        f.write(f\"Image Size: {CONFIG['IMG_HEIGHT']}x{CONFIG['IMG_WIDTH']}\\n\")\n",
        "        f.write(\"\\nCLASSIFICATION METRICS:\\n\")\n",
        "        f.write(report)\n",
        "\n",
        "    print(f\"Detailed Classification Report saved to {report_path}\")\n",
        "    print(report)\n",
        "\n",
        "# --- RUN REPORT GENERATION ---\n",
        "generate_report(model, val_generator, LOGS_DIR)\n",
        "\n",
        "# --- PLOT HISTORY (ACCURACY & LOSS) ---\n",
        "# This section now fills BOTH sides of the plot (Accuracy and Loss)\n",
        "if 'history' in locals():\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot 1: Accuracy (Left Side)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "    # Plot 2: Loss (Right Side) - FIXED: This was missing in your code\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Save the combined plot\n",
        "    plot_path = os.path.join(LOGS_DIR, 'training_history.png')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(plot_path)\n",
        "    print(f\"Training History Plot saved to {plot_path}\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No training history found in memory. (Did you restart the kernel?)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
