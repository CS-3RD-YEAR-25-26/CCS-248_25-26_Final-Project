{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556a53d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 50795,
     "status": "ok",
     "timestamp": 1765125523686,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "7556a53d",
    "outputId": "740f5bb4-05ce-453b-8b2e-f26e4e1f0da2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set dataset path (adjust if your folder is in a different location)\n",
    "DATA_DIR = '/content/drive/MyDrive/Tuberculosis-Detection/Dataset'\n",
    "\n",
    "# Verify dataset exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"‚úì Dataset found at: {DATA_DIR}\")\n",
    "    print(f\"  - Normal images: {len(os.listdir(os.path.join(DATA_DIR, 'Normal Chest X-rays')))}\")\n",
    "    print(f\"  - TB images: {len(os.listdir(os.path.join(DATA_DIR, 'TB Chest X-rays')))}\")\n",
    "else:\n",
    "    print(f\"‚úó Dataset NOT found at: {DATA_DIR}\")\n",
    "    print(\"Please upload your dataset to Google Drive first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad669da7",
   "metadata": {
    "id": "ad669da7"
   },
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f65ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9347,
     "status": "ok",
     "timestamp": 1765125533041,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "010f65ce",
    "outputId": "645f0138-28a4-43be-d0a6-b369fae6159d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úì GPU Available: {len(gpus)} device(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"GPU memory growth enabled\")\n",
    "else:\n",
    "    print(\"‚úó No GPU detected. Using CPU.\")\n",
    "    print(\"To enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f4d79",
   "metadata": {
    "id": "259f4d79"
   },
   "source": [
    "## Install Additional Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23906386",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4200,
     "status": "ok",
     "timestamp": 1765125537242,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "23906386",
    "outputId": "a166b691-4d93-4a96-e513-34a0f43def7e"
   },
   "outputs": [],
   "source": [
    "# Colab already has most packages, but let's ensure we have everything\n",
    "!pip install -q opencv-python-headless\n",
    "\n",
    "print(\"‚úì All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa426961",
   "metadata": {
    "id": "fa426961"
   },
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ac60f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1765125537560,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "d18ac60f",
    "outputId": "37331055-6b38-40a6-cc39-f468a872579d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"‚úì Mixed precision enabled (float16)\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d9e0f",
   "metadata": {
    "id": "345d9e0f"
   },
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde421da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1765125537580,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "dde421da",
    "outputId": "9f4df2dc-32cb-445b-f3d3-e11f62c02ea0"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Dataset paths\n",
    "    DATA_DIR = '/content/drive/MyDrive/Tuberculosis-Detection/Dataset'\n",
    "    NORMAL_DIR = os.path.join(DATA_DIR, \"Normal Chest X-rays\")\n",
    "    TB_DIR = os.path.join(DATA_DIR, \"TB Chest X-rays\")\n",
    "\n",
    "    # Output directories\n",
    "    OUTPUT_DIR = \"/content/outputs\"\n",
    "    MODEL_DIR = os.path.join(OUTPUT_DIR, \"models\")\n",
    "    PLOT_DIR = os.path.join(OUTPUT_DIR, \"plots\")\n",
    "\n",
    "    # Image parameters\n",
    "    IMG_SIZE = (224, 224)\n",
    "    IMG_HEIGHT, IMG_WIDTH = IMG_SIZE\n",
    "    CHANNELS = 3\n",
    "\n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 16  # Reduced for memory efficiency\n",
    "    EPOCHS = 30  # Increased for better learning\n",
    "    LEARNING_RATE = 2e-4  # Slightly higher initial learning rate\n",
    "\n",
    "    # Data split ratios\n",
    "    TRAIN_RATIO = 0.70\n",
    "    VAL_RATIO = 0.15\n",
    "    TEST_RATIO = 0.15\n",
    "\n",
    "    # Model parameters\n",
    "    L2_REG = 1e-4\n",
    "\n",
    "    # Callbacks\n",
    "    EARLY_STOPPING_PATIENCE = 8\n",
    "    LR_REDUCE_PATIENCE = 3\n",
    "    LR_REDUCE_FACTOR = 0.5\n",
    "\n",
    "    RANDOM_SEED = RANDOM_SEED\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(Config.MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(Config.PLOT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration set!\")\n",
    "print(f\"  - Image size: {Config.IMG_SIZE}\")\n",
    "print(f\"  - Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"  - Epochs: {Config.EPOCHS}\")\n",
    "print(f\"  - Learning rate: {Config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a40049",
   "metadata": {
    "id": "13a40049"
   },
   "source": [
    "## Data Loading and Preprocessing (Memory Efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6fe785",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1765125537748,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "3a6fe785",
    "outputId": "a732b675-31a6-45b1-9846-ab4d66030508"
   },
   "outputs": [],
   "source": [
    "def collect_image_paths(normal_dir, tb_dir):\n",
    "    \"\"\"\n",
    "    Collect file paths instead of loading images into memory\n",
    "    MEMORY EFFICIENT - Only stores paths, not image data\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COLLECTING IMAGE PATHS (Memory Efficient)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "\n",
    "    # Collect Normal image paths (label = 0)\n",
    "    if os.path.exists(normal_dir):\n",
    "        normal_files = [f for f in os.listdir(normal_dir)\n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        print(f\"Found {len(normal_files)} normal images\")\n",
    "\n",
    "        for filename in normal_files:\n",
    "            img_path = os.path.join(normal_dir, filename)\n",
    "            file_paths.append(img_path)\n",
    "            labels.append(0)\n",
    "\n",
    "    # Collect TB-positive image paths (label = 1)\n",
    "    if os.path.exists(tb_dir):\n",
    "        tb_files = [f for f in os.listdir(tb_dir)\n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        print(f\"Found {len(tb_files)} TB-positive images\")\n",
    "\n",
    "        for filename in tb_files:\n",
    "            img_path = os.path.join(tb_dir, filename)\n",
    "            file_paths.append(img_path)\n",
    "            labels.append(1)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    print(f\"\\n‚úì Total images found: {len(file_paths)}\")\n",
    "    print(f\"  - Normal: {np.sum(labels == 0)}\")\n",
    "    print(f\"  - TB-positive: {np.sum(labels == 1)}\")\n",
    "    print(f\"\\n‚úì Memory usage: Minimal (only paths stored, not images)\")\n",
    "\n",
    "    return file_paths, labels\n",
    "\n",
    "# Collect paths only (not loading images yet)\n",
    "file_paths, labels = collect_image_paths(\n",
    "    Config.NORMAL_DIR,\n",
    "    Config.TB_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c449544",
   "metadata": {
    "id": "8c449544"
   },
   "source": [
    "## Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7127cfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "executionInfo": {
     "elapsed": 13914,
     "status": "ok",
     "timestamp": 1765125551672,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "b7127cfc",
    "outputId": "72615451-c5fd-46c9-dc43-a7a7b90f526c"
   },
   "outputs": [],
   "source": [
    "def load_and_display_samples(file_paths, labels, num_samples=5):\n",
    "    \"\"\"\n",
    "    Load and display only a few sample images (memory efficient)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "\n",
    "    # Get indices for each class\n",
    "    normal_indices = np.where(labels == 0)[0][:num_samples]\n",
    "    tb_indices = np.where(labels == 1)[0][:num_samples]\n",
    "\n",
    "    # Load and display normal samples\n",
    "    for i, idx in enumerate(normal_indices):\n",
    "        img = cv2.imread(file_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, Config.IMG_SIZE)\n",
    "            axes[0, i].imshow(img, cmap='gray')\n",
    "            axes[0, i].set_title('Normal', fontsize=10)\n",
    "            axes[0, i].axis('off')\n",
    "\n",
    "    # Load and display TB samples\n",
    "    for i, idx in enumerate(tb_indices):\n",
    "        img = cv2.imread(file_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, Config.IMG_SIZE)\n",
    "            axes[1, i].imshow(img, cmap='gray')\n",
    "            axes[1, i].set_title('TB Positive', fontsize=10)\n",
    "            axes[1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(Config.PLOT_DIR, 'sample_images.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úì Sample images visualized!\")\n",
    "\n",
    "# Display samples\n",
    "load_and_display_samples(file_paths, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a946f823",
   "metadata": {
    "id": "a946f823"
   },
   "source": [
    "## Split Dataset (Patient-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7424e30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1765125551679,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "d7424e30",
    "outputId": "7c842e50-d7a8-4e58-92ca-ec049b082e67"
   },
   "outputs": [],
   "source": [
    "def split_dataset_paths(file_paths, labels, train_ratio, val_ratio, test_ratio):\n",
    "    \"\"\"\n",
    "    Split file paths (not loaded images) to avoid memory issues\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SPLITTING DATASET (PATIENT-LEVEL)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Convert to arrays\n",
    "    file_paths = np.array(file_paths)\n",
    "\n",
    "    # First split: separate test set\n",
    "    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
    "        file_paths,\n",
    "        labels,\n",
    "        test_size=test_ratio,\n",
    "        random_state=Config.RANDOM_SEED,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    # Second split: separate train and validation\n",
    "    val_ratio_adjusted = val_ratio / (train_ratio + val_ratio)\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        train_val_paths,\n",
    "        train_val_labels,\n",
    "        test_size=val_ratio_adjusted,\n",
    "        random_state=Config.RANDOM_SEED,\n",
    "        stratify=train_val_labels\n",
    "    )\n",
    "\n",
    "    print(f\"‚úì Training set: {len(train_paths)} images\")\n",
    "    print(f\"    Normal: {np.sum(train_labels==0)}, TB: {np.sum(train_labels==1)}\")\n",
    "    print(f\"‚úì Validation set: {len(val_paths)} images\")\n",
    "    print(f\"    Normal: {np.sum(val_labels==0)}, TB: {np.sum(val_labels==1)}\")\n",
    "    print(f\"‚úì Test set: {len(test_paths)} images\")\n",
    "    print(f\"    Normal: {np.sum(test_labels==0)}, TB: {np.sum(test_labels==1)}\")\n",
    "\n",
    "    return {\n",
    "        'train': (train_paths, train_labels),\n",
    "        'val': (val_paths, val_labels),\n",
    "        'test': (test_paths, test_labels)\n",
    "    }\n",
    "\n",
    "# Split the data (paths only)\n",
    "data_splits = split_dataset_paths(\n",
    "    file_paths, labels,\n",
    "    Config.TRAIN_RATIO, Config.VAL_RATIO, Config.TEST_RATIO\n",
    ")\n",
    "\n",
    "train_paths, train_labels = data_splits['train']\n",
    "val_paths, val_labels = data_splits['val']\n",
    "test_paths, test_labels = data_splits['test']\n",
    "\n",
    "# Save test paths for later\n",
    "test_data = {'paths': test_paths, 'labels': test_labels}\n",
    "test_data_path = os.path.join(Config.OUTPUT_DIR, 'test_data.npz')\n",
    "np.savez(test_data_path, **test_data)\n",
    "print(f\"\\n‚úì Test data paths saved to: {test_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ea1a1",
   "metadata": {
    "id": "765ea1a1"
   },
   "source": [
    "## Data Augmentation (Memory Efficient with tf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01d537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5024,
     "status": "ok",
     "timestamp": 1765125556704,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "6b01d537",
    "outputId": "8371eab1-8517-4d52-dd2c-e8a2137e0716"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(file_path, label, img_size, augment=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single image (called by tf.data pipeline)\n",
    "    \"\"\"\n",
    "    # Read image file\n",
    "    img = tf.io.read_file(file_path)\n",
    "\n",
    "    # Try multiple decoders for compatibility\n",
    "    try:\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "    except:\n",
    "        try:\n",
    "            img = tf.image.decode_png(img, channels=3)\n",
    "        except:\n",
    "            img = tf.image.decode_image(img, channels=3)\n",
    "\n",
    "    # Resize\n",
    "    img = tf.image.resize(img, img_size)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "\n",
    "    if augment:\n",
    "        # Random augmentations (all native TensorFlow operations)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_brightness(img, 0.15)\n",
    "        img = tf.image.random_contrast(img, 0.85, 1.15)\n",
    "        img = tf.image.random_saturation(img, 0.9, 1.1)\n",
    "\n",
    "        # Random zoom simulation (crop and resize)\n",
    "        if tf.random.uniform([]) > 0.5:\n",
    "            crop_size = tf.random.uniform([], 0.85, 1.0)\n",
    "            crop_h = tf.cast(img_size[0] * crop_size, tf.int32)\n",
    "            crop_w = tf.cast(img_size[1] * crop_size, tf.int32)\n",
    "            img = tf.image.random_crop(img, [crop_h, crop_w, 3])\n",
    "            img = tf.image.resize(img, img_size)\n",
    "\n",
    "    return img, label\n",
    "\n",
    "def create_tf_dataset(file_paths, labels, batch_size, img_size, augment=False, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create memory-efficient tf.data.Dataset pipeline\n",
    "    MEMORY EFFICIENT - Loads images on-the-fly during training\n",
    "    \"\"\"\n",
    "    # Create dataset from file paths\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000, seed=Config.RANDOM_SEED)\n",
    "\n",
    "    # Map loading and preprocessing (parallel processing)\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: load_and_preprocess_image(x, y, img_size, augment),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "    # Batch and prefetch\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING MEMORY-EFFICIENT DATA PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create datasets (no tensorflow-addons needed!)\n",
    "train_dataset = create_tf_dataset(\n",
    "    train_paths, train_labels,\n",
    "    Config.BATCH_SIZE, Config.IMG_SIZE,\n",
    "    augment=True, shuffle=True\n",
    ")\n",
    "\n",
    "val_dataset = create_tf_dataset(\n",
    "    val_paths, val_labels,\n",
    "    Config.BATCH_SIZE, Config.IMG_SIZE,\n",
    "    augment=False, shuffle=False\n",
    ")\n",
    "\n",
    "test_dataset = create_tf_dataset(\n",
    "    test_paths, test_labels,\n",
    "    Config.BATCH_SIZE, Config.IMG_SIZE,\n",
    "    augment=False, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"‚úì Memory-efficient pipeline created!\")\n",
    "print(\"  - Images loaded on-the-fly (not stored in RAM)\")\n",
    "print(\"  - Training augmentation: flip, brightness, contrast, saturation, zoom\")\n",
    "print(\"  - Parallel loading with prefetching\")\n",
    "print(f\"  - Batch size: {Config.BATCH_SIZE}\")\n",
    "print(\"  - Using native TensorFlow ops (no external dependencies)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280b759",
   "metadata": {
    "id": "1280b759"
   },
   "source": [
    "## Build Custom CNN Model (From Scratch - Enhanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef536d7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2443,
     "status": "ok",
     "timestamp": 1765125559149,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "ef536d7b",
    "outputId": "768f8af7-0acd-4daf-e2ad-f99ba6aa7cdd"
   },
   "outputs": [],
   "source": [
    "def build_tb_cnn_model(input_shape, l2_reg=1e-4):\n",
    "    \"\"\"\n",
    "    Build ENHANCED custom CNN from scratch for TB detection\n",
    "    NO TRANSFER LEARNING - Built entirely from scratch\n",
    "    IMPROVED ARCHITECTURE for better accuracy (70-80%+)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BUILDING ENHANCED CNN MODEL FROM SCRATCH\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model = models.Sequential(name='TB_CNN_Detector_Enhanced')\n",
    "\n",
    "    # Block 1\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(128, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Block 3\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # Block 4\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.Conv2D(512, (3, 3), padding='same',\n",
    "                           kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Activation('relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Global Average Pooling\n",
    "    model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "    # Dense layers with more capacity\n",
    "    model.add(layers.Dense(512, activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid', dtype='float32'))\n",
    "\n",
    "    print(\"‚úì ENHANCED Model architecture:\")\n",
    "    print(\"  - 4 Convolutional Blocks (64->128->256->512 filters)\")\n",
    "    print(\"  - Extra conv layer in Block 3 for deeper features\")\n",
    "    print(\"  - Progressive Dropout: 0.25 ‚Üí 0.5\")\n",
    "    print(\"  - L2 Regularization: 1e-4\")\n",
    "    print(\"  - 2 Dense layers (512 + 256) for better classification\")\n",
    "    print(\"  - Batch Normalization throughout\")\n",
    "    print(\"  - Global Average Pooling\")\n",
    "    print(\"  - Binary Classification Output\")\n",
    "    print(\"\\n  EXPECTED ACCURACY: 70-85% with proper training\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "input_shape = (Config.IMG_HEIGHT, Config.IMG_WIDTH, Config.CHANNELS)\n",
    "model = build_tb_cnn_model(input_shape, Config.L2_REG)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6b7fe",
   "metadata": {
    "id": "ddc6b7fe"
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5678ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1765125559183,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "6e5678ca",
    "outputId": "d5d4eb96-a92b-4bb5-fffa-6a498e961dfb"
   },
   "outputs": [],
   "source": [
    "# Calculate class weights for imbalanced data\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class Weights:\")\n",
    "print(f\"  - Normal: {class_weight_dict[0]:.4f}\")\n",
    "print(f\"  - TB: {class_weight_dict[1]:.4f}\")\n",
    "\n",
    "# Compile model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=Config.LEARNING_RATE)\n",
    "loss = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(curve='PR', name='auc_pr')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Model compiled!\")\n",
    "print(f\"  - Optimizer: Adam (lr={Config.LEARNING_RATE})\")\n",
    "print(f\"  - Loss: Binary Cross-Entropy\")\n",
    "print(f\"  - Metrics: Accuracy, Precision, Recall, AUC, AUC-PR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767199c",
   "metadata": {
    "id": "5767199c"
   },
   "source": [
    "## Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f035ef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1765125559203,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "98f035ef",
    "outputId": "5b315e3e-3e53-405f-d1f5-fe1173000336"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=Config.EARLY_STOPPING_PATIENCE,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "\n",
    "    # Reduce learning rate\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        factor=Config.LR_REDUCE_FACTOR,\n",
    "        patience=Config.LR_REDUCE_PATIENCE,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "\n",
    "    # Model checkpoint\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(Config.MODEL_DIR, 'best_tb_cnn_model.h5'),\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úì Callbacks configured:\")\n",
    "print(f\"  - EarlyStopping: patience={Config.EARLY_STOPPING_PATIENCE}\")\n",
    "print(f\"  - ReduceLROnPlateau: factor={Config.LR_REDUCE_FACTOR}\")\n",
    "print(f\"  - ModelCheckpoint: best_tb_cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7491b699",
   "metadata": {
    "id": "7491b699"
   },
   "source": [
    "## Train the Model\n",
    "\n",
    "**This will take 30-60 minutes with GPU enabled.**\n",
    "\n",
    "You can monitor:\n",
    "- Loss decreasing\n",
    "- Accuracy increasing\n",
    "- **AUC (primary metric)** - should reach > 0.80\n",
    "- **Recall (Sensitivity)** - Critical for TB detection\n",
    "\n",
    "**MEMORY EFFICIENT:** Images loaded on-the-fly, not stored in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede600f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2199829,
     "status": "ok",
     "timestamp": 1765127759041,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "ede600f2",
    "outputId": "b580fe97-bd5d-44a5-81aa-ffc7dcc893ec"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING (MEMORY EFFICIENT)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Training on: {len(train_paths)} images\")\n",
    "print(f\"Validating on: {len(val_paths)} images\")\n",
    "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {Config.EPOCHS}\")\n",
    "print(f\"Learning rate: {Config.LEARNING_RATE}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_paths) // Config.BATCH_SIZE\n",
    "validation_steps = len(val_paths) // Config.BATCH_SIZE\n",
    "\n",
    "print(f\"\\nSteps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "# Train the model with tf.data pipeline (memory efficient)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=Config.EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee2fdb3",
   "metadata": {
    "id": "0ee2fdb3"
   },
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7d4ce1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "executionInfo": {
     "elapsed": 2768,
     "status": "ok",
     "timestamp": 1765127761808,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "da7d4ce1",
    "outputId": "0b91ae26-100f-4108-a3c5-fccbb6078c14"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'auc']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    if metric in history.history:\n",
    "        axes[idx].plot(history.history[metric], label=f'Train {metric}')\n",
    "        axes[idx].plot(history.history[f'val_{metric}'], label=f'Val {metric}')\n",
    "        axes[idx].set_title(f'{metric.capitalize()} vs Epochs')\n",
    "        axes[idx].set_xlabel('Epoch')\n",
    "        axes[idx].set_ylabel(metric.capitalize())\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(Config.PLOT_DIR, 'training_history.png')\n",
    "plt.savefig(plot_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Training history saved to: {plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c22ff5",
   "metadata": {
    "id": "f4c22ff5"
   },
   "source": [
    "## Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa69e1e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1765127761999,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "fa69e1e3",
    "outputId": "47889191-1fca-47b1-bd75-157e38708aa6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Final model saved: /content/outputs/models/tb_cnn_model.h5\n",
      "‚úì Model architecture saved: /content/outputs/models/model_architecture.json\n",
      "‚úì Training config saved: /content/outputs/models/training_config.json\n"
     ]
    }
   ],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(Config.MODEL_DIR, 'tb_cnn_model.h5')\n",
    "model.save(final_model_path)\n",
    "print(f\"‚úì Final model saved: {final_model_path}\")\n",
    "\n",
    "# Save model architecture as JSON\n",
    "json_path = os.path.join(Config.MODEL_DIR, 'model_architecture.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "print(f\"‚úì Model architecture saved: {json_path}\")\n",
    "\n",
    "# Save training configuration\n",
    "config_dict = {\n",
    "    'img_size': Config.IMG_SIZE,\n",
    "    'batch_size': Config.BATCH_SIZE,\n",
    "    'epochs': Config.EPOCHS,\n",
    "    'learning_rate': Config.LEARNING_RATE,\n",
    "    'l2_regularization': Config.L2_REG,\n",
    "}\n",
    "config_path = os.path.join(Config.MODEL_DIR, 'training_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "print(f\"‚úì Training config saved: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821ac49",
   "metadata": {
    "id": "9821ac49"
   },
   "source": [
    "## Download Trained Model to Your Computer\n",
    "\n",
    "Run this cell to download the trained model and outputs to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1554547",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 7433,
     "status": "ok",
     "timestamp": 1765127769439,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "c1554547",
    "outputId": "f6b93023-4a8f-4930-8bca-b43520f5bff7"
   },
   "outputs": [],
   "source": [
    "# Create a zip file of all outputs\n",
    "!cd /content && zip -r outputs.zip outputs/\n",
    "\n",
    "# Download the zip file\n",
    "from google.colab import files\n",
    "files.download('/content/outputs.zip')\n",
    "\n",
    "print(\"‚úì Outputs downloaded!\")\n",
    "print(\"Extract the zip file on your computer to access:\")\n",
    "print(\"  - best_tb_cnn_model.h5 (best model)\")\n",
    "print(\"  - tb_cnn_model.h5 (final model)\")\n",
    "print(\"  - training_history.png\")\n",
    "print(\"  - test_data.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4324028",
   "metadata": {
    "id": "b4324028"
   },
   "source": [
    "## Quick Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b0f16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218244,
     "status": "ok",
     "timestamp": 1765127987689,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "779b0f16",
    "outputId": "fd3a0439-f206-4218-d005-348aef0abde1"
   },
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = keras.models.load_model(os.path.join(Config.MODEL_DIR, 'best_tb_cnn_model.h5'))\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Evaluate with tf.data pipeline\n",
    "test_results = best_model.evaluate(test_dataset, verbose=1)\n",
    "\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  - Loss: {test_results[0]:.4f}\")\n",
    "print(f\"  - Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"  - Precision: {test_results[2]:.4f}\")\n",
    "print(f\"  - Recall (Sensitivity): {test_results[3]:.4f} ‚Üê PRIMARY METRIC\")\n",
    "print(f\"  - AUC: {test_results[4]:.4f}\")\n",
    "print(f\"  - AUC-PR: {test_results[5]:.4f}\")\n",
    "\n",
    "# Make predictions (load images for confusion matrix)\n",
    "print(\"\\nGenerating predictions for confusion matrix...\")\n",
    "y_pred_proba = best_model.predict(test_dataset).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, y_pred, target_names=['Normal', 'TB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30967a7",
   "metadata": {
    "id": "b30967a7"
   },
   "source": [
    "## Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download your trained model** (cell 16)\n",
    "2. **Run evaluation script** on your local machine: `python evaluate_model.py`\n",
    "3. **Generate Grad-CAM visualizations**: `python gradcam_visualization.py`\n",
    "4. **Convert to TFLite**: `python convert_to_tflite.py`\n",
    "\n",
    "### Key Files in outputs.zip:\n",
    "- `best_tb_cnn_model.h5` - Best model based on validation AUC\n",
    "- `tb_cnn_model.h5` - Final model after all epochs\n",
    "- `test_data.npz` - Test set for evaluation\n",
    "- `training_history.png` - Training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xy2GD-UYpWUT",
   "metadata": {
    "id": "Xy2GD-UYpWUT"
   },
   "source": [
    "---\n",
    "\n",
    "# CONTINUE TRAINING YOUR MODEL\n",
    "\n",
    "Run this cell to load your trained model and continue training for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLIb7gxHpeQM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328448,
     "status": "ok",
     "timestamp": 1765128316150,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "qLIb7gxHpeQM",
    "outputId": "7cd44abb-97dc-4b5d-adb6-b33df23c5b62"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONTINUE TRAINING FROM EXISTING MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Load your already trained model\n",
    "model_path = os.path.join(Config.MODEL_DIR, 'best_tb_cnn_model.h5')\n",
    "loaded_model = keras.models.load_model(model_path)\n",
    "print(f\"‚úì Model loaded from: {model_path}\")\n",
    "\n",
    "# 2. Set new training parameters (you can adjust these)\n",
    "ADDITIONAL_EPOCHS = 10      # Train for 10 more epochs\n",
    "NEW_LEARNING_RATE = 1e-4    # Lower learning rate (optional)\n",
    "\n",
    "# 3. Recompile with new (or same) learning rate\n",
    "loaded_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=NEW_LEARNING_RATE),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[\n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(curve='PR', name='auc_pr')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model recompiled with learning rate: {NEW_LEARNING_RATE}\")\n",
    "print(f\"‚úì Will train for {ADDITIONAL_EPOCHS} more epochs\")\n",
    "\n",
    "# 4. Setup callbacks\n",
    "callbacks_continued = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        patience=5,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_auc',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode='max',\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(Config.MODEL_DIR, 'continued_tb_model.h5'),\n",
    "        monitor='val_auc',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING CONTINUED TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 5. Continue training!\n",
    "history_continued = loaded_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=ADDITIONAL_EPOCHS,\n",
    "    steps_per_epoch=len(train_paths) // Config.BATCH_SIZE,\n",
    "    validation_steps=len(val_paths) // Config.BATCH_SIZE,\n",
    "    callbacks=callbacks_continued,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì CONTINUED TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"‚úì New model saved to: continued_tb_model.h5\")\n",
    "print(f\"‚úì You can now evaluate this model on the test set\")\n",
    "\n",
    "# Quick comparison\n",
    "print(\"\\nüìä Quick Validation Results:\")\n",
    "val_results = loaded_model.evaluate(val_dataset, verbose=0)\n",
    "print(f\"  - Val Loss: {val_results[0]:.4f}\")\n",
    "print(f\"  - Val Accuracy: {val_results[1]:.4f}\")\n",
    "print(f\"  - Val AUC: {val_results[4]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crmlHX_z34m2",
   "metadata": {
    "id": "crmlHX_z34m2"
   },
   "source": [
    "---\n",
    "\n",
    "## Continue Training ANY Model (Iterative Retraining)\n",
    "\n",
    "Use this cell to keep training **any** model you've already trained.\n",
    "- Train `continued_tb_model.h5` again\n",
    "- Train the original model again\n",
    "- Train any other saved model\n",
    "\n",
    "Just update the `MODEL_TO_LOAD` variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mvIvgo9N36Xj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350103,
     "status": "ok",
     "timestamp": 1765130016204,
     "user": {
      "displayName": "Frederick Jibril Bunag",
      "userId": "08104108974320537896"
     },
     "user_tz": -480
    },
    "id": "mvIvgo9N36Xj",
    "outputId": "14f520f6-81c8-40cd-b1e8-e16187431c7d"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ITERATIVE RETRAINING - TRAIN ANY MODEL AGAIN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# üìù CHANGE THIS to train different models:\n",
    "# Options: 'best_tb_cnn_model.h5', 'continued_tb_model.h5', 'tb_cnn_model.h5', etc.\n",
    "MODEL_TO_LOAD = 'continued_tb_model_v3.h5'  # ‚Üê Change this to train a different model\n",
    "\n",
    "# Training configuration (adjust these as needed)\n",
    "EPOCHS_THIS_ROUND = 10           # How many more epochs to train\n",
    "LEARNING_RATE_THIS_ROUND = 5e-5  # Lower LR for each iteration (5e-5, then 1e-5, etc.)\n",
    "OUTPUT_MODEL_NAME = 'continued_tb_model_v4.h5'  # ‚Üê Name for the output model\n",
    "\n",
    "print(f\"Loading model: {MODEL_TO_LOAD}\")\n",
    "print(f\"Will train for: {EPOCHS_THIS_ROUND} epochs\")\n",
    "print(f\"Learning rate: {LEARNING_RATE_THIS_ROUND}\")\n",
    "print(f\"Output will be saved as: {OUTPUT_MODEL_NAME}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the specified model\n",
    "model_path = os.path.join(Config.MODEL_DIR, MODEL_TO_LOAD)\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"\\n‚ùå ERROR: Model '{MODEL_TO_LOAD}' not found at {model_path}\")\n",
    "    print(\"\\nAvailable models in your directory:\")\n",
    "    if os.path.exists(Config.MODEL_DIR):\n",
    "        model_files = [f for f in os.listdir(Config.MODEL_DIR) if f.endswith('.h5')]\n",
    "        for i, mf in enumerate(model_files, 1):\n",
    "            print(f\"   {i}. {mf}\")\n",
    "    else:\n",
    "        print(\"   (Model directory not found)\")\n",
    "else:\n",
    "    # Load model\n",
    "    model_to_retrain = keras.models.load_model(model_path)\n",
    "    print(f\"\\n‚úì Model loaded successfully from: {MODEL_TO_LOAD}\")\n",
    "\n",
    "    # Recompile with new learning rate\n",
    "    model_to_retrain.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_THIS_ROUND),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc'),\n",
    "            keras.metrics.AUC(curve='PR', name='auc_pr')\n",
    "        ]\n",
    "    )\n",
    "    print(f\"‚úì Recompiled with learning rate: {LEARNING_RATE_THIS_ROUND}\")\n",
    "\n",
    "    # Setup callbacks\n",
    "    callbacks_iterative = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_auc',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            verbose=1,\n",
    "            mode='max',\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(Config.MODEL_DIR, OUTPUT_MODEL_NAME),\n",
    "            monitor='val_auc',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING ITERATIVE TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Train!\n",
    "    history_iterative = model_to_retrain.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS_THIS_ROUND,\n",
    "        steps_per_epoch=len(train_paths) // Config.BATCH_SIZE,\n",
    "        validation_steps=len(val_paths) // Config.BATCH_SIZE,\n",
    "        callbacks=callbacks_iterative,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"‚úì ITERATIVE TRAINING COMPLETED!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"‚úì Model saved as: {OUTPUT_MODEL_NAME}\")\n",
    "\n",
    "    # Show results\n",
    "    print(\"\\nüìä Validation Results After This Round:\")\n",
    "    val_results = model_to_retrain.evaluate(val_dataset, verbose=0)\n",
    "    print(f\"  - Val Loss: {val_results[0]:.4f}\")\n",
    "    print(f\"  - Val Accuracy: {val_results[1]:.4f}\")\n",
    "    print(f\"  - Val Precision: {val_results[2]:.4f}\")\n",
    "    print(f\"  - Val Recall: {val_results[3]:.4f}\")\n",
    "    print(f\"  - Val AUC: {val_results[4]:.4f}\")\n",
    "\n",
    "    print(\"\\nüí° To train this model again:\")\n",
    "    print(f\"   1. Change MODEL_TO_LOAD to '{OUTPUT_MODEL_NAME}'\")\n",
    "    print(f\"   2. Lower the learning rate (e.g., 1e-5 or 5e-6)\")\n",
    "    print(f\"   3. Change OUTPUT_MODEL_NAME to 'continued_tb_model_v3.h5'\")\n",
    "    print(\"   4. Run this cell again!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
